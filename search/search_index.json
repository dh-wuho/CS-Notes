{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This is my CS Notes.","title":"Home"},{"location":"csc541/","text":"","title":"Preface"},{"location":"csc541/interval-tree/","text":"The interval tree structure stores a set of intervals and returns for any query key all the intervals that contain this query value. Suppose the underlying set of intervals is the set {[a1, b1],[a2, b2],...,[an, bn]}. Let T be any balanced search tree for the set of interval endpoints {a1, a2,...,an, b1,...,bn}. Each interval [ai, bi] of our set is now stored in a node that satisfies 1. the key of the node is contained in [ai, bi], and 2. the interval [ai, bi] is contained in the interval associated with the node. This node might not be unique; if during this descent the key of the current node occurs as endpoint of the interval, then some node below the current node will also satisfy both properties. For the interval tree structure, it makes no difference which node we choose. To implement it, we need two different types of nodes: the search-tree nodes augmented by the left and right list pointers, and the list nodes. // list node typedef struct ls_n_t { key_t key; struct ls_n_t *next; object_t *object; } list_node_t; // tree node typedef struct tr_n_t { key_t key; struct tr_n_t *left; struct tr_n_t *right; list_node_t *left_list; list_node_t *right_list; /* balancing information */ } tree_node_t; Theorem. The interval tree structure is a static data structure that can be built in time O(n log n) and needs space O(n). It lists all intervals containing a given query key in output-sensitive time O(log n + k) if there are k such intervals. Why not a dynamic structure? 1. Insertion of an interval at a node involves insertion into two ordered lists of endpoints. For each list (when established using a BST with a doubly linked list of leaves), O(k) time to list the first k elements and O(log n) time for insertion and deletion. Restructuring of the tree to make it balanced requires correction of associated lists at each affected node. Requires joining and separation of ordered lists: No known way to achieve this in sublinear time! So, Building a tree for a superset of all expected interval endpoints, if known, can be an option, and it still requires search trees for the left and right lists in each node!","title":"Interval Tree"},{"location":"csc541/segment-tree/","text":"The primary task performed by a segment tree is the same as that done by an interval tree: keeping track of a set of n intervals, here assumed to be halfopen, and listing for a given query key all the intervals that contain that key in output-sensitive time O(log n + k) if the output consisted of k intervals. Assume a setX = {x1, . . . , xn} of key values and a search tree T for {\u2212\u221e} \u222a X. Any interval [xi, xj [ can be expressed in many ways as union of node intervals,4 so it can be represented by subsets of the tree nodes. Theorem. The canonical interval decomposition is a representation of the interval as union of disjoint node intervals. Any search path for a value in the interval will go through exactly one node that belongs to the canonical interval decomposition. The canonical interval decomposition is easy to construct. We start with the interval [xi, xj [ at the root: 1. Each time the node interval of the current node is entirely contained in [xi, xj [, we take that node into our representation and stop following that path down because all nodes below are redundant; 2. Each time the node interval of the current node partially overlaps [xi, xj [, we follow both paths down; 3. Each time the node interval of the current node is disjoint from [xi, xj [, we stop following that path down. Theorem. Let X = {x1, . . . , xn} be a set of key values and T a search tree for {\u2212\u221e} \u222a X. Then for any interval bounded by values from X, the canonical decomposition has size at most 2 height(T ) and can be constructed in time O (height(T )). If T is of height O(log n), the canonical interval decomposition has size O(log n) and can be found in time O(log n). Theorem. The segment tree structure is a static data structure that can be built in time O(n log n) and needs space O(n log n). It lists all intervals containing a given query key in output-sensitive time O(log n + k) if there are k such intervals. To implement the segment tree structure, we again need two types of nodes \u2013 the tree nodes and the interval lists attached to each tree node. typedef struct ls_n_t { /* interval [a,b[ */ key_t key_a, key_b; struct ls_n_t *next; object_t *object; } list_node_t; typedef struct tr_n_t { key_t key; struct tr_n_t *left; struct tr_n_t *right; list_node_t *interval_list; /* balancing information */ } tree_node_t; Interval containment queries: Nither the root nor any node on the left or right boundary path of the tree, can have any intervals of the canonical interval decomposition. Node intervals are unbounded, and we are representing only finite intervals. Nodes near the leaf level will have nonempty lists. Interval trees tended to store intervals in higher-up nodes. Segment tree is static structure. Problems in Making it Dynamic Allow insertion and deletion in each node. There are O(log n) fragments of the canonical decomposition for single insert or delete. efficient O(log n) - use a search tree only for the first fragment, and doubly linked list for the remaining fragments. allowing O(1) insertion and deletion of intervals. Rebalancing of the underlying tree by rotations causes changes in the lists attached to the tree nodes no efficient solution, better than for interval trees, since the sequence of the intervals attached to a tree node does not matter. allows a representation of the sets of intervals attached to the nodes, which makes segment trees truly dynamic.","title":"Segment tree"},{"location":"csc541/segment-tree/#problems-in-making-it-dynamic","text":"Allow insertion and deletion in each node. There are O(log n) fragments of the canonical decomposition for single insert or delete. efficient O(log n) - use a search tree only for the first fragment, and doubly linked list for the remaining fragments. allowing O(1) insertion and deletion of intervals. Rebalancing of the underlying tree by rotations causes changes in the lists attached to the tree nodes no efficient solution, better than for interval trees, since the sequence of the intervals attached to a tree node does not matter. allows a representation of the sets of intervals attached to the nodes, which makes segment trees truly dynamic.","title":"Problems in Making it Dynamic"},{"location":"csc541/trees-for-union-intervals/","text":"Measure Tree Measure each node n contains the measure n->measure of the union of all node intervals of nodes in the subtree below n For any node n, this information can easily be reconstructed from its lower neighbors: { if n->interval list \u0007= NULL, then n->measure is the length of the node interval of n; { if n is a leaf and n->interval list = NULL, then n->measure is 0; { if n is an interior node and n->interval list = NULL, then n->measure = n->left->measure + n->right->measure. Dynamic Measure Tree A fully dynamic structure to maintain the measure of a union of intervals is the measure tree defined by Gonnet, Munro, and Wood (1983). The construction of the measure tree begins with any balanced search tree on the endpoints of all intervals in the current set and \u2212\u221e. The associated intervals of a node are all those intervals in the current set that have at least one endpoint in the node interval; like the node interval, we do not store the associated intervals in the node, but just need them as concept. Each node n of the search tree contains three additional fields: { n->measure is the measure of the intersection of the node interval of n with the union of all its associated intervals. { n->rightmax is the maximum right endpoint of all intervals associated with n. { n->leftmin is the minimum left endpoint of all intervals associated with n. For any interior node n, this information can be reconstructed from its lower neighbors. Two of the fields are easy: { n->rightmax = max(n->left->rightmax, n->right->rightmax), and { n->leftmin = min(n->left->leftmin, n->right->leftmin). if $l$ and $r$ are the left and right endpoints of the node interval of n, we have 1. if n->right->leftmin < l and n->left->rightmax \u2265 r, n->measure = r \u2212 l; 2. if n->right->leftmin \u2265 l and n->left->rightmax \u2265 r, n->measure = (r \u2212 n->key) + n->left->measure; 3. if n->right->leftmin < l and n->left->rightmax < r, n->measure = n->right->measure + (n->key \u2212 l); and 4. if n->right->leftmin \u2265 l and n->left->rightmax < r, n->measure = n->right->measure + n->left->measure. Theorem. The measure tree structure is a dynamic data structure that keeps track of a set of n intervals, supporting insertion and deletion of intervals in time O(log n), and that answers queries for the measure of the union of the intervals in O(1) time. The structure has size O(n).","title":"Trees for union intervals"},{"location":"csc541/trees-for-union-intervals/#measure-tree","text":"","title":"Measure Tree"},{"location":"csc541/trees-for-union-intervals/#measure","text":"each node n contains the measure n->measure of the union of all node intervals of nodes in the subtree below n For any node n, this information can easily be reconstructed from its lower neighbors: { if n->interval list \u0007= NULL, then n->measure is the length of the node interval of n; { if n is a leaf and n->interval list = NULL, then n->measure is 0; { if n is an interior node and n->interval list = NULL, then n->measure = n->left->measure + n->right->measure.","title":"Measure"},{"location":"csc541/trees-for-union-intervals/#dynamic-measure-tree","text":"A fully dynamic structure to maintain the measure of a union of intervals is the measure tree defined by Gonnet, Munro, and Wood (1983). The construction of the measure tree begins with any balanced search tree on the endpoints of all intervals in the current set and \u2212\u221e. The associated intervals of a node are all those intervals in the current set that have at least one endpoint in the node interval; like the node interval, we do not store the associated intervals in the node, but just need them as concept. Each node n of the search tree contains three additional fields: { n->measure is the measure of the intersection of the node interval of n with the union of all its associated intervals. { n->rightmax is the maximum right endpoint of all intervals associated with n. { n->leftmin is the minimum left endpoint of all intervals associated with n. For any interior node n, this information can be reconstructed from its lower neighbors. Two of the fields are easy: { n->rightmax = max(n->left->rightmax, n->right->rightmax), and { n->leftmin = min(n->left->leftmin, n->right->leftmin). if $l$ and $r$ are the left and right endpoints of the node interval of n, we have 1. if n->right->leftmin < l and n->left->rightmax \u2265 r, n->measure = r \u2212 l; 2. if n->right->leftmin \u2265 l and n->left->rightmax \u2265 r, n->measure = (r \u2212 n->key) + n->left->measure; 3. if n->right->leftmin < l and n->left->rightmax < r, n->measure = n->right->measure + (n->key \u2212 l); and 4. if n->right->leftmin \u2265 l and n->left->rightmax < r, n->measure = n->right->measure + n->left->measure. Theorem. The measure tree structure is a dynamic data structure that keeps track of a set of n intervals, supporting insertion and deletion of intervals in time O(log n), and that answers queries for the measure of the union of the intervals in O(1) time. The structure has size O(n).","title":"Dynamic Measure Tree"},{"location":"csc568/0-index/","text":"","title":"Preface"},{"location":"csc568/10-SAN/","text":"SAN(Storage Area Network) Organize connection between storage and servers . SCSI SCSI is a kind of block protocol. Others are files or objects. iSCSI iSCSI, FCOE fiber channel over Ethernet substitution of fiber channel transport protocol(block protocol) SCSI, HiPPI, ESCO switched fabric block I/O // to do ![SANarchictecture][] HBA Host Bus Adapter: cable, hub, switch About SAN Channel Oriented channel processor Network Oriented full multiplexing peer to peer internetwork Benefits of SAN high bandwidth SCSI extension resource consolidation centralized storage, management security(it's the server initiates the I/O) scalability","title":"10 - SAN"},{"location":"csc568/10-SAN/#sanstorage-area-network","text":"Organize connection between storage and servers .","title":"SAN(Storage Area Network)"},{"location":"csc568/10-SAN/#scsi","text":"SCSI is a kind of block protocol. Others are files or objects.","title":"SCSI"},{"location":"csc568/10-SAN/#iscsi","text":"iSCSI, FCOE fiber channel over Ethernet substitution of fiber channel transport protocol(block protocol) SCSI, HiPPI, ESCO switched fabric block I/O // to do ![SANarchictecture][]","title":"iSCSI"},{"location":"csc568/10-SAN/#hba","text":"Host Bus Adapter: cable, hub, switch","title":"HBA"},{"location":"csc568/10-SAN/#about-san","text":"Channel Oriented channel processor Network Oriented full multiplexing peer to peer internetwork","title":"About SAN"},{"location":"csc568/10-SAN/#benefits-of-san","text":"high bandwidth SCSI extension resource consolidation centralized storage, management security(it's the server initiates the I/O) scalability","title":"Benefits of SAN"},{"location":"csc568/11-NAS/","text":"NAS(Network Attached Storage) File-based and client-server model. Protocol for NAS well-defined message and response support OS file operations Sketch client: initialize the request server: locate file, response the file through network, resolve conflict, prioritize design: what APIs are needed to finish the jobs? (straightforward protocol) how elaborate the client/server is? (server has to handle many clients at the same time) so the simpler server is, the more concurrency server can be. e.g. JavaScript. Client translate users' calls to NAS protocol cooperate with server heart beat message cache(latency-tolerance device) Server: manage data(efficiently, reliably, securely...) could prefetch could upcalls(communicate with clients). upcalls are limited on stateful protocol Protocols of NAS NFS stateless hierarchy no upcalss from servers SMB(CIFS) more \"talktive\" generic: printer","title":"11 - NAS"},{"location":"csc568/11-NAS/#nasnetwork-attached-storage","text":"File-based and client-server model.","title":"NAS(Network Attached Storage)"},{"location":"csc568/11-NAS/#protocol-for-nas","text":"well-defined message and response support OS file operations","title":"Protocol for NAS"},{"location":"csc568/11-NAS/#sketch","text":"client: initialize the request server: locate file, response the file through network, resolve conflict, prioritize design: what APIs are needed to finish the jobs? (straightforward protocol) how elaborate the client/server is? (server has to handle many clients at the same time) so the simpler server is, the more concurrency server can be. e.g. JavaScript.","title":"Sketch"},{"location":"csc568/11-NAS/#client","text":"translate users' calls to NAS protocol cooperate with server heart beat message cache(latency-tolerance device)","title":"Client"},{"location":"csc568/11-NAS/#server","text":"manage data(efficiently, reliably, securely...) could prefetch could upcalls(communicate with clients). upcalls are limited on stateful protocol","title":"Server:"},{"location":"csc568/11-NAS/#protocols-of-nas","text":"NFS stateless hierarchy no upcalss from servers SMB(CIFS) more \"talktive\" generic: printer","title":"Protocols of NAS"},{"location":"csc568/12-DFS/","text":"DFS(Distributed File System) How to compare different file systems? What's the measurements of DFS? 1. View hierarchical FS global view: all clients have the same view local view: client can have different views, mount 2. State open on clients also open on server(connection) server must remember client must close comparison(statefull vs stateless) message is longer(stateless) server is simpler idempotent(stateless) lock or lease stateful -> client locks a file stateless -> lease a period of time 3. Cache Where does the caching occur? clients: memory/disks server: memory Cache Size Choice: once it's determined, changing it is risky!(Cache background....any solution to it) Cache Replacement Policy: LRU, approximate policy - clock Consistency: replication, update Cache Consistency Policy: UNIX file consistency: no design on it,it leaves to the application to handle the consistency problem. e.g. Google doc. write-through-cache(relatively write-through-disk) dealyed-write: delay some files write-on-close: delay one single file delayed-write-on-close: the lifetime of files are usually short 4. Replication block-based access raw block: typically high performance applicaiton must provide all data manage leverage: e.g. ls file clustered file server file-based access Virtualization. Benefits over block-based access: abstraction hides the implementation server independence Fine-grained data management permission backup(e.g. S3) - share data - cost high CPU per request network technology infi band RDMA, rget, rput","title":"12 - DFS"},{"location":"csc568/12-DFS/#dfsdistributed-file-system","text":"How to compare different file systems? What's the measurements of DFS?","title":"DFS(Distributed File System)"},{"location":"csc568/12-DFS/#1-view","text":"hierarchical FS global view: all clients have the same view local view: client can have different views, mount","title":"1. View"},{"location":"csc568/12-DFS/#2-state","text":"open on clients also open on server(connection) server must remember client must close comparison(statefull vs stateless) message is longer(stateless) server is simpler idempotent(stateless) lock or lease stateful -> client locks a file stateless -> lease a period of time","title":"2. State"},{"location":"csc568/12-DFS/#3-cache","text":"Where does the caching occur? clients: memory/disks server: memory Cache Size Choice: once it's determined, changing it is risky!(Cache background....any solution to it) Cache Replacement Policy: LRU, approximate policy - clock Consistency: replication, update Cache Consistency Policy: UNIX file consistency: no design on it,it leaves to the application to handle the consistency problem. e.g. Google doc. write-through-cache(relatively write-through-disk) dealyed-write: delay some files write-on-close: delay one single file delayed-write-on-close: the lifetime of files are usually short","title":"3. Cache"},{"location":"csc568/12-DFS/#4-replication","text":"","title":"4. Replication"},{"location":"csc568/12-DFS/#block-based-access","text":"raw block: typically high performance applicaiton must provide all data manage leverage: e.g. ls file clustered file server","title":"block-based access"},{"location":"csc568/12-DFS/#file-based-access","text":"Virtualization. Benefits over block-based access: abstraction hides the implementation server independence Fine-grained data management permission backup(e.g. S3) - share data - cost high CPU per request network technology infi band RDMA, rget, rput","title":"file-based access"},{"location":"csc568/13-Ceph/","text":"Ceph A kind of distributed file system, using CRUSH algorithm . Goal scalability capacity throughtput client performance individual emphasison HPO: shared files, lock-step manner(distribute jobs to processors then merge) reliability dynamic(peta byte) build incrementally failures quality and character of workload changes performance Key Ideas objects based storage system decouple data and metadata They are in different clusters They have different retreving pattern Archictecture Overveiw Key features Servers decouple data and metadata CRUSH: controlled repliction under scalable washing files striped onto predictable objects CRUSH maps objects to storage device Dynamically distribute meta data management metadata operations make up 50% of all operations dynamic subtree partitioning Object based storage Others: migrate, replication, failure detection, recovery Clients ceph interface POSIX compliant decouple data and metadata user space implementation >example: >client sends request to MDS >MDS returns capability, file index, size, stripe information... >client reads/writes directly from/to OSD >MDS mnage the capability >client sends close synchronization adhere to POSIX include HPC-oriented extensions consistency/correctness by defaults optimally relax constraint, like clients could write to different locations extension for both data and metadata synchronize I/O used on multiple writes, or mix reader and writer. distribute metadata HDS used journaling repetition metadata updates in memory optimize on-disk layout for read access adaptively distributed cached metadata accessed nodes distributed objected storage files are splited into objects objects are members of placement groups placement groups are distributed to OSD","title":"13 - Ceph"},{"location":"csc568/13-Ceph/#ceph","text":"A kind of distributed file system, using CRUSH algorithm .","title":"Ceph"},{"location":"csc568/13-Ceph/#goal","text":"scalability capacity throughtput client performance individual emphasison HPO: shared files, lock-step manner(distribute jobs to processors then merge) reliability dynamic(peta byte) build incrementally failures quality and character of workload changes performance","title":"Goal"},{"location":"csc568/13-Ceph/#key-ideas","text":"objects based storage system decouple data and metadata They are in different clusters They have different retreving pattern","title":"Key Ideas"},{"location":"csc568/13-Ceph/#archictecture-overveiw","text":"","title":"Archictecture Overveiw"},{"location":"csc568/13-Ceph/#key-features","text":"","title":"Key features"},{"location":"csc568/13-Ceph/#servers","text":"decouple data and metadata CRUSH: controlled repliction under scalable washing files striped onto predictable objects CRUSH maps objects to storage device Dynamically distribute meta data management metadata operations make up 50% of all operations dynamic subtree partitioning Object based storage Others: migrate, replication, failure detection, recovery","title":"Servers"},{"location":"csc568/13-Ceph/#clients","text":"ceph interface POSIX compliant decouple data and metadata user space implementation >example: >client sends request to MDS >MDS returns capability, file index, size, stripe information... >client reads/writes directly from/to OSD >MDS mnage the capability >client sends close synchronization adhere to POSIX include HPC-oriented extensions consistency/correctness by defaults optimally relax constraint, like clients could write to different locations extension for both data and metadata synchronize I/O used on multiple writes, or mix reader and writer.","title":"Clients"},{"location":"csc568/13-Ceph/#distribute-metadata","text":"HDS used journaling repetition metadata updates in memory optimize on-disk layout for read access adaptively distributed cached metadata accessed nodes distributed objected storage files are splited into objects objects are members of placement groups placement groups are distributed to OSD","title":"distribute metadata"},{"location":"csc568/14-GFS/","text":"GFS(Google File System) What does it do? set out to build a DFS willing to change anything(e.g. client API: gfs-open, gfs-read...) apps modification/cooperation Design constraints compliant to failure in norm bugs, human errors, power loss maintaining, detecting and recovery files are huge multi-GB are common. Comparing to billions of KB files most modification are appends random writes are practically non-exist many files are written once and read many times sequentially two types of reads large streaming read small random reads google gathers websites and build inddddd indice. skip and forward, always in forwad dimention. sustained bandwidth 7 latency Architectual Design GFS cluster a single master multiple chunk servers chunk servers can be accessed by clients compliant linux servers File represent as fix-sized chunk(like object in Ceph) 64-bits unique ID stored at chunk server(deliver directly from chunk server, reducing throughput latency) 3 way replication Master metadata management Clients master the metadata data from chunk servers it doesn't use VFS no caching at clients. I/O are usually streaming, no temporary cache ..... Archictecture Overveiw Why Single Master Design? simple. Multi-masters have to cooperate or replicate master. master only ensuadsf chunk locations clients typically ask for multiple chunk locations in one request chunk size 64MB or 64bits(ID) fewer requests to master fewer metadata entries fragmentation(64MB per chunk) 64bits per 64MB chunk, cache chunk ID in memory chunk location no persistent: clients just know the chunks but not their locations startup: send info to master operation logs metadata updates are logs take global snapshots to truncate logs Lease 60s time-out renew indefinitely","title":"14 - GFS"},{"location":"csc568/14-GFS/#gfsgoogle-file-system","text":"","title":"GFS(Google File System)"},{"location":"csc568/14-GFS/#what-does-it-do","text":"set out to build a DFS willing to change anything(e.g. client API: gfs-open, gfs-read...) apps modification/cooperation","title":"What does it do?"},{"location":"csc568/14-GFS/#design-constraints","text":"compliant to failure in norm bugs, human errors, power loss maintaining, detecting and recovery files are huge multi-GB are common. Comparing to billions of KB files most modification are appends random writes are practically non-exist many files are written once and read many times sequentially two types of reads large streaming read small random reads google gathers websites and build inddddd indice. skip and forward, always in forwad dimention. sustained bandwidth 7 latency","title":"Design constraints"},{"location":"csc568/14-GFS/#architectual-design","text":"GFS cluster a single master multiple chunk servers chunk servers can be accessed by clients compliant linux servers File represent as fix-sized chunk(like object in Ceph) 64-bits unique ID stored at chunk server(deliver directly from chunk server, reducing throughput latency) 3 way replication Master metadata management Clients master the metadata data from chunk servers it doesn't use VFS no caching at clients. I/O are usually streaming, no temporary cache .....","title":"Architectual Design"},{"location":"csc568/14-GFS/#archictecture-overveiw","text":"","title":"Archictecture Overveiw"},{"location":"csc568/14-GFS/#why-single-master-design","text":"simple. Multi-masters have to cooperate or replicate master. master only ensuadsf chunk locations clients typically ask for multiple chunk locations in one request chunk size 64MB or 64bits(ID) fewer requests to master fewer metadata entries fragmentation(64MB per chunk) 64bits per 64MB chunk, cache chunk ID in memory chunk location no persistent: clients just know the chunks but not their locations startup: send info to master operation logs metadata updates are logs take global snapshots to truncate logs","title":"Why Single Master Design?"},{"location":"csc568/14-GFS/#lease","text":"60s time-out renew indefinitely","title":"Lease"}]}