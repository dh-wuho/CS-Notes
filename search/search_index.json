{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"This is my CS Notes.","title":"Home"},{"location":"2019-Fall/csc510-review/","text":"Engineering Basics 1. Setup 2. Shells Essential commands. ls : list content of a directory. cd : change directories to a new path. mkdir : make a new directory. pwd : output current directory cp : copy files rm : rm files touch : make a new file/update status** cat : output the contents of a file. head : output the first lines of a file. tail : output the last lines of a file. grep : search files for a key phrase. wget : retrieve file from the web. cut : extract output of a file (columns) awk and sed : Magic commands for extracting, searching, and transforming content. # Create a graphical directory tree from your current directory ls -R | grep \":$\" | sed -e 's/:$//' -e 's/[^-][^\\/]*\\//--/g' -e 's/^/ /' -e 's/-/|/' # Command can run sequentially or conditionally: command1 ; command2 # do command1 and command2 independently and sequentially (command1 ; command2) # in a sub-shell command1 || command2 # do command2 only if command1 fails command1 && command2 # do command2 only if command1 succeeds # Pipes and redirects change standard in and standard out from defaults. command # default standard in and standard out command < inputFile # redirect of inputFile contents to command as standard in command > outputFile # redirect command output to outputFile as standard out command1 | command2 # pipes output of command1 as standard in to command2 command & # run in background, typically used for applications 3. Version Control with Git 4. Markdown and IDES 5. Virtual Environments 6. Task Management REST APIs GET, POST, PUT, DELETE Bots A bot is an agent of automation. A bot can perform automated, repetitive, predefined tasks. Relationship between CLI and Bots Bots can simplify - complexity in complex tools (command line tools) - integration between complex systems - deployment and configuration (no app store; no install, passwords, or setup) Bots Risks Information/interactions not discoverable Interactions may be ephemeral Reduced opportunity for learning Direct manipulation better for complex tasks May bring new disruptions and complexities Ethical and moral training Design and UX Methods Wireframes A wireframe is a view schematic that captures all layout and content decisions of that view. Storyboards A storyboard illustrates the timeline of user performing a task as a sequence of frames. Personas A persona is arch-user type which represents a segment of a user population, and allows role-play during task planning and UX design. Flow Maps A flow map describes the wayfinding activity of a user and transitions between UI states. Other Design Methods Software Design Diagrams Drawing and diagramming is essential task in software development. Types: Class Diagrams, Sequence Diagrams, State Diagrams UML (Unified Modeling Language) 2.0 A standard for modeling object-oriented software. Evaluation: intention-revealing naming, Single Responsibility. Design Patterns Design patterns are descriptions of communicating objects and classes that are customized to solve a general design problem in a particular context. The design pattern identifies the participating classes and instances, their roles and collaborations, and the distribution of responsibilities Pattern Families Creational: Concerned with the process of object creation Structural: Deal with the composition of classes or objects Behavioral: Characterize the ways in which classes or objects interact and distribute responsibility Some Patterns Singleton Pattern Visitor Pattern Builder Pattern Abstract Factory Pattern Adapter Pattern Strategy Pattern Architecture Data-Centered Repository: Data held in central database is accessible by all components. Blackboard: Blackboard is used as a global database for sharing different information as input data, partial solutions, alternatives and final solutions. Knowledge source, Blackboard, Control component drives Call and Return Main and Subprogram Object-Oriented Layered Model Description Data-Flow Pipe-And-Filter: Components (Filters), Connectors (Pipes) Batch Sequential: Each component completes transformation of input before passing to output. Software Metrics and Refactoring Root Canal Refactoring Painful, expensive, the result of long periods of neglect. When: Refactoring for protracted periods; time specifically set aside. Why: Typically after code has gotten difficult to maintain. Not considered best practice.Studies suggest that it is not common. Floss Refactoring Easy to do, regular, something people know they should do. When: Continuously; Think \"every few minutes\". Why: It helps achieve an immediate goal. Considered best practice. Studies suggest that this is common. Code Metrics Lines of Code Halstead Complexity: Number of operations and symbols in code. Cyclomatic Complexity: Number of independent paths in program. Dep Degree: Number of data flow paths in code. OO Metrics Weighted Methods Per Class: essentially number of methods. Cohesion: Degree to which the tasks performed by a single module are functionally related. Coupling: Measure of interdependence between two objects. Attribute Hiding Factor: Measure of the proportion of attributes that are \"invisible\" from other classes or objects Method Hiding Factor: Measure of the proportion of methods that are \"invisible\" from other classes or objects Summary Process Metrics Velocity: Units of work completed per iteration. Burndown: Work done versus outstanding work overtime. Fault-slippage: #Bugs making it into production. Introduction to Testing Types of Testing Unit Testing Confirm that the component or subsystem is correctly coded and carries out the intended functionality Integration Testing Test the interfaces among the subsystems. System Testing Determine if the system meets the requirements (functional and nonfunctional) Acceptance Testing Demonstrate that the system meets the requirements and is ready to use. Styles of Testing Black Box Testing Focus: I/O behavior Goal: Reduce number of test cases by equivalence partitioning White Box Testing Advantages: - Based on the code: can be measured objectively and automatically - Can be used to compare test suites - Allows for covering the coded behaviors Different kinds: - Control-flow based - Data-flow based - fault based TDD(Test-Driven Development) Write tests before writing code Tests are automated Often use \"x\" Unit framework Must run at 100% before proceeding BDD(Behavior-Driven Development) example-based communication with testers, managers Features and scenarios written in plain-text Tools can execute features as automated tests. Goals: - Improve feature understanding - Describe how system behaves rather than how it works - Must run at 100% before proceeding Software Processes The Roots of Agile Individuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan Stories Card - Conversation - Confirmation Attributes of an user story or epic INVEST - Independent - Negotiable - Valuable for purchasers or users - Estimate-able - Small - Testable Extreme programming practice: Corollary Real Customer Involvement Incremental Deployment Team Continuity Shrinking Team Root-Cause Analysis Shared Code Code & Test Single Code Base Daily Deployment Negotiated Scope Contract Lean Principles Eliminate Waste Create Knowledge Build Quality In Defer Commitment Deliver Fast Respect People Improve the System Continuous Integration A practice where developers automatically build, test, and analyze a software change in response to every software change committed to the source repository. A continuous integration server monitors the status of every commit and reports any problems. Claimed Benefits Detecting defects and fixing them faster Health of software is measurable Reduce assumptions about environment Reducing repetitive processes saves time, costs, and effort CI can enable you to release deployable software at any point in time. CI offers a global mechanism for feedback about failure, enabling developers to have more confidence in making their changes. Principles Commit Code Frequently Don\u2019t commit broken code Fix broken builds immediately Write automated developer tools All tests and inspections must pass Run private builds Avoid getting broken code Risks of Not doing CI Little or no confidence in whether we could even build the software Lengthy integration phases before delivering the software internally (to test teams) or externally (customer), during which nothing else got done Inability to produce and reproduce testable builds Fear of making changes or refactoring the database or code Difficulty in populating the database with diff. sets of data Difficulty in maintaining development and testing environments Late discovery of defects Lack of project visibility (awareness of changes) Low-quality software Configuration Management Traditional Configuration Management (CM) Identify all items related to software. Manage changes to those items. Enable variations of items and changes. Maintain quality of versions and releases. Provide traceability between changes and requirements. Modern Configuration Management In traditional configuration management, the process is not fully triggered until deployment. In modern configuration management, lightweight CM is integrated throughout the software process. Package Manager Binary: apt-get, choco (windows), brew (mac) Source: bower(web), nuget(C#), pip(python), npm(node.js), maven(Java), RubyGems Task Manager Tools: make, ivy, ant, grunt Tasks: Validation, testing, tool chains(antlr example), less to css translation, deployment, staging Configuration Management Concepts agentless: Operations can be applied to remote client without any local service running. configuration polling: Client queries periodically master server for new configuration state. idempotent: Applying operations multiple times will not affect result - (e.g. if python is already installed, script will not fail). orchestration: Controlling schedule of change rollout while ensuring operational stability. Configuration Tools Ansible, Puppet, Chef, Productivity Personal Productivity TODO Deliberate Work Personal Software Process (PSP) Team Software Process (TSP) Social Software Processes","title":"csc510 Review"},{"location":"2019-Fall/csc510-review/#engineering-basics","text":"","title":"Engineering Basics"},{"location":"2019-Fall/csc510-review/#1-setup","text":"","title":"1. Setup"},{"location":"2019-Fall/csc510-review/#2-shells","text":"","title":"2. Shells"},{"location":"2019-Fall/csc510-review/#essential-commands","text":"ls : list content of a directory. cd : change directories to a new path. mkdir : make a new directory. pwd : output current directory cp : copy files rm : rm files touch : make a new file/update status** cat : output the contents of a file. head : output the first lines of a file. tail : output the last lines of a file. grep : search files for a key phrase. wget : retrieve file from the web. cut : extract output of a file (columns) awk and sed : Magic commands for extracting, searching, and transforming content. # Create a graphical directory tree from your current directory ls -R | grep \":$\" | sed -e 's/:$//' -e 's/[^-][^\\/]*\\//--/g' -e 's/^/ /' -e 's/-/|/' # Command can run sequentially or conditionally: command1 ; command2 # do command1 and command2 independently and sequentially (command1 ; command2) # in a sub-shell command1 || command2 # do command2 only if command1 fails command1 && command2 # do command2 only if command1 succeeds # Pipes and redirects change standard in and standard out from defaults. command # default standard in and standard out command < inputFile # redirect of inputFile contents to command as standard in command > outputFile # redirect command output to outputFile as standard out command1 | command2 # pipes output of command1 as standard in to command2 command & # run in background, typically used for applications","title":"Essential commands."},{"location":"2019-Fall/csc510-review/#3-version-control-with-git","text":"","title":"3. Version Control with Git"},{"location":"2019-Fall/csc510-review/#4-markdown-and-ides","text":"","title":"4. Markdown and IDES"},{"location":"2019-Fall/csc510-review/#5-virtual-environments","text":"","title":"5. Virtual Environments"},{"location":"2019-Fall/csc510-review/#6-task-management","text":"","title":"6. Task Management"},{"location":"2019-Fall/csc510-review/#rest-apis","text":"GET, POST, PUT, DELETE","title":"REST APIs"},{"location":"2019-Fall/csc510-review/#bots","text":"A bot is an agent of automation. A bot can perform automated, repetitive, predefined tasks.","title":"Bots"},{"location":"2019-Fall/csc510-review/#relationship-between-cli-and-bots","text":"Bots can simplify - complexity in complex tools (command line tools) - integration between complex systems - deployment and configuration (no app store; no install, passwords, or setup)","title":"Relationship between CLI and Bots"},{"location":"2019-Fall/csc510-review/#bots-risks","text":"Information/interactions not discoverable Interactions may be ephemeral Reduced opportunity for learning Direct manipulation better for complex tasks May bring new disruptions and complexities Ethical and moral training","title":"Bots Risks"},{"location":"2019-Fall/csc510-review/#design-and-ux-methods","text":"","title":"Design and UX Methods"},{"location":"2019-Fall/csc510-review/#wireframes","text":"A wireframe is a view schematic that captures all layout and content decisions of that view.","title":"Wireframes"},{"location":"2019-Fall/csc510-review/#storyboards","text":"A storyboard illustrates the timeline of user performing a task as a sequence of frames.","title":"Storyboards"},{"location":"2019-Fall/csc510-review/#personas","text":"A persona is arch-user type which represents a segment of a user population, and allows role-play during task planning and UX design.","title":"Personas"},{"location":"2019-Fall/csc510-review/#flow-maps","text":"A flow map describes the wayfinding activity of a user and transitions between UI states.","title":"Flow Maps"},{"location":"2019-Fall/csc510-review/#other-design-methods","text":"","title":"Other Design Methods"},{"location":"2019-Fall/csc510-review/#software-design","text":"","title":"Software Design"},{"location":"2019-Fall/csc510-review/#diagrams","text":"Drawing and diagramming is essential task in software development. Types: Class Diagrams, Sequence Diagrams, State Diagrams","title":"Diagrams"},{"location":"2019-Fall/csc510-review/#uml-unified-modeling-language-20","text":"A standard for modeling object-oriented software. Evaluation: intention-revealing naming, Single Responsibility.","title":"UML (Unified Modeling Language) 2.0"},{"location":"2019-Fall/csc510-review/#design-patterns","text":"Design patterns are descriptions of communicating objects and classes that are customized to solve a general design problem in a particular context. The design pattern identifies the participating classes and instances, their roles and collaborations, and the distribution of responsibilities","title":"Design Patterns"},{"location":"2019-Fall/csc510-review/#pattern-families","text":"Creational: Concerned with the process of object creation Structural: Deal with the composition of classes or objects Behavioral: Characterize the ways in which classes or objects interact and distribute responsibility","title":"Pattern Families"},{"location":"2019-Fall/csc510-review/#some-patterns","text":"Singleton Pattern Visitor Pattern Builder Pattern Abstract Factory Pattern Adapter Pattern Strategy Pattern","title":"Some Patterns"},{"location":"2019-Fall/csc510-review/#architecture","text":"","title":"Architecture"},{"location":"2019-Fall/csc510-review/#data-centered","text":"Repository: Data held in central database is accessible by all components. Blackboard: Blackboard is used as a global database for sharing different information as input data, partial solutions, alternatives and final solutions. Knowledge source, Blackboard, Control component drives","title":"Data-Centered"},{"location":"2019-Fall/csc510-review/#call-and-return","text":"Main and Subprogram Object-Oriented Layered Model Description","title":"Call and Return"},{"location":"2019-Fall/csc510-review/#data-flow","text":"Pipe-And-Filter: Components (Filters), Connectors (Pipes) Batch Sequential: Each component completes transformation of input before passing to output.","title":"Data-Flow"},{"location":"2019-Fall/csc510-review/#software-metrics-and-refactoring","text":"","title":"Software Metrics and Refactoring"},{"location":"2019-Fall/csc510-review/#root-canal-refactoring","text":"Painful, expensive, the result of long periods of neglect. When: Refactoring for protracted periods; time specifically set aside. Why: Typically after code has gotten difficult to maintain. Not considered best practice.Studies suggest that it is not common.","title":"Root Canal Refactoring"},{"location":"2019-Fall/csc510-review/#floss-refactoring","text":"Easy to do, regular, something people know they should do. When: Continuously; Think \"every few minutes\". Why: It helps achieve an immediate goal. Considered best practice. Studies suggest that this is common.","title":"Floss Refactoring"},{"location":"2019-Fall/csc510-review/#code-metrics","text":"Lines of Code Halstead Complexity: Number of operations and symbols in code. Cyclomatic Complexity: Number of independent paths in program. Dep Degree: Number of data flow paths in code.","title":"Code Metrics"},{"location":"2019-Fall/csc510-review/#oo-metrics","text":"Weighted Methods Per Class: essentially number of methods. Cohesion: Degree to which the tasks performed by a single module are functionally related. Coupling: Measure of interdependence between two objects. Attribute Hiding Factor: Measure of the proportion of attributes that are \"invisible\" from other classes or objects Method Hiding Factor: Measure of the proportion of methods that are \"invisible\" from other classes or objects","title":"OO Metrics"},{"location":"2019-Fall/csc510-review/#summary","text":"","title":"Summary"},{"location":"2019-Fall/csc510-review/#process-metrics","text":"Velocity: Units of work completed per iteration. Burndown: Work done versus outstanding work overtime. Fault-slippage: #Bugs making it into production.","title":"Process Metrics"},{"location":"2019-Fall/csc510-review/#introduction-to-testing","text":"","title":"Introduction to Testing"},{"location":"2019-Fall/csc510-review/#types-of-testing","text":"Unit Testing Confirm that the component or subsystem is correctly coded and carries out the intended functionality Integration Testing Test the interfaces among the subsystems. System Testing Determine if the system meets the requirements (functional and nonfunctional) Acceptance Testing Demonstrate that the system meets the requirements and is ready to use.","title":"Types of Testing"},{"location":"2019-Fall/csc510-review/#styles-of-testing","text":"","title":"Styles of Testing"},{"location":"2019-Fall/csc510-review/#black-box-testing","text":"Focus: I/O behavior Goal: Reduce number of test cases by equivalence partitioning","title":"Black Box Testing"},{"location":"2019-Fall/csc510-review/#white-box-testing","text":"Advantages: - Based on the code: can be measured objectively and automatically - Can be used to compare test suites - Allows for covering the coded behaviors Different kinds: - Control-flow based - Data-flow based - fault based","title":"White Box Testing"},{"location":"2019-Fall/csc510-review/#tddtest-driven-development","text":"Write tests before writing code Tests are automated Often use \"x\" Unit framework Must run at 100% before proceeding","title":"TDD(Test-Driven Development)"},{"location":"2019-Fall/csc510-review/#bddbehavior-driven-development","text":"example-based communication with testers, managers Features and scenarios written in plain-text Tools can execute features as automated tests. Goals: - Improve feature understanding - Describe how system behaves rather than how it works - Must run at 100% before proceeding","title":"BDD(Behavior-Driven Development)"},{"location":"2019-Fall/csc510-review/#software-processes","text":"","title":"Software Processes"},{"location":"2019-Fall/csc510-review/#the-roots-of-agile","text":"Individuals and interactions over processes and tools Working software over comprehensive documentation Customer collaboration over contract negotiation Responding to change over following a plan","title":"The Roots of Agile"},{"location":"2019-Fall/csc510-review/#stories","text":"Card - Conversation - Confirmation","title":"Stories"},{"location":"2019-Fall/csc510-review/#attributes-of-an-user-story-or-epic","text":"INVEST - Independent - Negotiable - Valuable for purchasers or users - Estimate-able - Small - Testable","title":"Attributes of an user story or epic"},{"location":"2019-Fall/csc510-review/#extreme-programming-practice-corollary","text":"Real Customer Involvement Incremental Deployment Team Continuity Shrinking Team Root-Cause Analysis Shared Code Code & Test Single Code Base Daily Deployment Negotiated Scope Contract","title":"Extreme programming practice: Corollary"},{"location":"2019-Fall/csc510-review/#lean-principles","text":"Eliminate Waste Create Knowledge Build Quality In Defer Commitment Deliver Fast Respect People Improve the System","title":"Lean Principles"},{"location":"2019-Fall/csc510-review/#continuous-integration","text":"A practice where developers automatically build, test, and analyze a software change in response to every software change committed to the source repository. A continuous integration server monitors the status of every commit and reports any problems.","title":"Continuous Integration"},{"location":"2019-Fall/csc510-review/#claimed-benefits","text":"Detecting defects and fixing them faster Health of software is measurable Reduce assumptions about environment Reducing repetitive processes saves time, costs, and effort CI can enable you to release deployable software at any point in time. CI offers a global mechanism for feedback about failure, enabling developers to have more confidence in making their changes.","title":"Claimed Benefits"},{"location":"2019-Fall/csc510-review/#principles","text":"Commit Code Frequently Don\u2019t commit broken code Fix broken builds immediately Write automated developer tools All tests and inspections must pass Run private builds Avoid getting broken code","title":"Principles"},{"location":"2019-Fall/csc510-review/#risks-of-not-doing-ci","text":"Little or no confidence in whether we could even build the software Lengthy integration phases before delivering the software internally (to test teams) or externally (customer), during which nothing else got done Inability to produce and reproduce testable builds Fear of making changes or refactoring the database or code Difficulty in populating the database with diff. sets of data Difficulty in maintaining development and testing environments Late discovery of defects Lack of project visibility (awareness of changes) Low-quality software","title":"Risks of Not doing CI"},{"location":"2019-Fall/csc510-review/#configuration-management","text":"","title":"Configuration Management"},{"location":"2019-Fall/csc510-review/#traditional-configuration-management-cm","text":"Identify all items related to software. Manage changes to those items. Enable variations of items and changes. Maintain quality of versions and releases. Provide traceability between changes and requirements.","title":"Traditional Configuration Management (CM)"},{"location":"2019-Fall/csc510-review/#modern-configuration-management","text":"In traditional configuration management, the process is not fully triggered until deployment. In modern configuration management, lightweight CM is integrated throughout the software process.","title":"Modern Configuration Management"},{"location":"2019-Fall/csc510-review/#package-manager","text":"Binary: apt-get, choco (windows), brew (mac) Source: bower(web), nuget(C#), pip(python), npm(node.js), maven(Java), RubyGems","title":"Package Manager"},{"location":"2019-Fall/csc510-review/#task-manager","text":"Tools: make, ivy, ant, grunt Tasks: Validation, testing, tool chains(antlr example), less to css translation, deployment, staging","title":"Task Manager"},{"location":"2019-Fall/csc510-review/#configuration-management-concepts","text":"agentless: Operations can be applied to remote client without any local service running. configuration polling: Client queries periodically master server for new configuration state. idempotent: Applying operations multiple times will not affect result - (e.g. if python is already installed, script will not fail). orchestration: Controlling schedule of change rollout while ensuring operational stability.","title":"Configuration Management Concepts"},{"location":"2019-Fall/csc510-review/#configuration-tools","text":"Ansible, Puppet, Chef,","title":"Configuration Tools"},{"location":"2019-Fall/csc510-review/#productivity","text":"","title":"Productivity"},{"location":"2019-Fall/csc510-review/#personal-productivity","text":"TODO Deliberate Work","title":"Personal Productivity"},{"location":"2019-Fall/csc510-review/#personal-software-process-psp","text":"","title":"Personal Software Process (PSP)"},{"location":"2019-Fall/csc510-review/#team-software-process-tsp","text":"","title":"Team Software Process (TSP)"},{"location":"2019-Fall/csc510-review/#social-software-processes","text":"","title":"Social Software Processes"},{"location":"2019-Fall/finalReview/","text":"Including pre-midterm concepts: cross validation, conditional probability, Bayesian Rules and so on. BN: Bayesian Network, D-seperation, factoring the joint distribution, and Bayesian Network Inference. Linear Regression: Minimize sum of squared errors ANN: Explain and evaluate neural network classification methods. SVM: Explain the notions of maximum margin hyperplanes and support vectors and their roles and uses in support vector machines. Clustering: \u2013 Explain, evaluate, and compare k-means, hierarchical density-based DBSCAN clustering methods. \u2013 Explain and compare different methods for measuring the similarity of clusters. \u2013 Explain and compare methods for evaluating the quality of clusters produced by clustering methods. Associate Analysis: \u2013 Explain and compare the notions of support and confidence of association rules. \u2013 Explain and compare the notion of frequent itemsets with the notions of maximal and closed itemsets. \u2013 Explain the Apriori principle in identifying association rules. \u2013 Generate and calculate frequent itemsets using the Apriori algorithm. Hash-Table storage and transaction comparison. FP tree: \u2013 Explain the FP tree representations and algorithms for frequent pattern analysis, and compare with the basic Apriori algorithm. \u2013 Compute the FP tree representation of a database of itemsets. Identify and explain how the Apriori principle applies to generating association rules from itemsets. Explain, motivate, compare, and compute measures of the interestingness of associations and association rules.","title":"csc522 Final Review"},{"location":"2019-Fall/midTerm/","text":"1. List, explain, and identify the different types of data attributes (nominal, ordinal, interval, ratio). Nominal: ==, != Ordinal: ==, !=, <, >, <=, >= Interval: ==, !=, <, >, <=, >=, +, - Ratio: ==, !=, <, >, <=, >=, +, -, *, / 2. Explain the difference between supervised and unsupervised learning. Supervised: feature X with Label Y to predict(Regression, Classification). Unsupervised: feature X to learn itself to describe(Density estimation, Clustering, Dimensionality reduction). 3. Explain the nature and significance of noise and outliers in data analysis. Noise refers to modification of original values. Outliers are data objects with characteristics that are considerably different than most of the other data objects in the data set. 4. List, explain, compare, and evaluate methods for handling missing data values. Eliminate Data Objects: straightforward to implement, but signi cantly limits the scope and power of study, and introduces bias when data is not missing at random. Estimate Missing Values: Comparing to complete-case analysis, it can utilize all present data for analysis. However, some data imputation methods are only suitable when missing rate is low, e.g. mean- or median- lling and knn. Ignore the Missing Value During Analysis. Replace with all possible values(weighted by probabilities) 5. Explain the notion of sampling, and list and explain potential problems arising in sampling data sets; Sampling is the use of a subset of the population to represent the whole population. Two key principles for effective sampling: - using a sample will work almost as well as using the entire data sets, if the sample is representative. A sample is representative if it has approximately the same property as the original set of data. 6. Explain and understand how Principle Component Analysis(PCA) works and can be applied to do feature reduction; When applying PCA, the resulted principle components are always orthogonal(perpendicular) to one another. When applying PCA, each of principal components is always a linear combination of the original features. 7. List, explain, and compare different methods for converting continuous attributes into discrete attributes. Discretization Attributes Transformation 8. Explain and compute the mean, median, modes, and Z-scores of sets of data values. Frequency(\u9891\u7387\u6216\u9891\u6570): Mean(\u5e73\u5747\u6570): very sensitive to outliers Median(\u4e2d\u4f4d\u6570): Summary of frequency distribution Modes(\u4f17\u6570): Z-scores: the standard score is the signed fractional number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured. $$Z-scores = (oneData - mean) / standardDeviation$$ 9. Explain the notion of probability distribution and compute simple distributions from data frequencies. 10. Explain the notion of conditional probability and compute conditional probabilities from probability distributions. 11. Explain the notion of correlation of data attributes. Correlation measures the linear relationship between objects. Variance \u2013 measure of the deviation from the mean for points in one dimension. Covariance \u2013 a measure of how much each of the dimensions varies from the mean with respect to each other. $$Correlation(X, Y) = \\frac{Covariance(X, Y)}{Variance(X)Variance(Y)}$$ 12. Explain and give examples of the notion of decision trees. Given a collection of records (training set ): Each record contains a set of attributes, one of the attributes is the class. (Categorical, discrete, unordered) Find a model for class attribute as a function of the values of other attributes. Goal: previously unseen records (test set) should be assigned a class as accurately as possible. 13. Construct decision trees from small data sets by using entropy and information gain. 14. Compare alternative splitting attributes in decision tree construction by applying gini or entropy measures. 15. Explain the problems caused by underfitting and overfitting data, and by inexpressive representations. Underfitting: when model is too simple, both training and test errors are large. Overfitting: when model is complex where training error is small but test error is large. 16. Understand and explain: generalization error, optimistic error, pessimistic error; postpruning based on optimistic error, pessimistic error. Generalization error: Generalization error is the true error for the population of examples we would like to optimize. Optimistic error: Pessimistic: 17. How to build decision trees with missing values and apply the decision tree to test data with missing values. 18. Chi-square Tests $$X^2 = \\sum{\\frac{O_i - E_i}{E_i}}$$ O is pratical value. E is theoretical value. 19. Explain the procedure of Hold-out, Stratified Sampling, Cross-Validation, and LOOCV. Holdout: 2/3 for training and 1/3 for testing. Stratified Sampling: sample in such a way that each class is represented in both sets. Cross-Validation LOOCV 20. Contrast and explain the notions of error rate and confusion matrices in classification. 21. Use confusion and cost matrices to compute which of two classifiers is better for a data set. 22. Explain the notion of Accuracy, Error Rate, Precision, Recall, F-measure. $$Accuracy = \\frac{TP+TN}{TP+FP+TN+FN}$$ $$Error Rate = 1 - Accuracy$$ $$Precision(p) = \\frac{TP}{TP+FP}$$ $$Recall(r) = \\frac{TP}{TP+FN}$$ $$F-Measure(F) = \\frac{2rp}{r+p} = \\frac{2TP}{2TP+FP+FN}$$ The Accuracy (or error rate=1-Accuracy) is an inadequate measure of the performance of an algorithm, it doesn\u2019t take into account the cost of making wrong decisions. 23. Explain the notion of an ROC curve, AUC, and its meaning for classifier performance. 24. Use ROC curves to compare performance of different classifiers. 25. Understanding the procedure of Bagging and Boosting, especially the Adaboost. 26. Explain and understand how the bagging and AdaBoost works. 27. Explain how the K-nearest Neighbour classifier works. 28. Explain the notion of probabilistic or Bayesian methods. 29. State and explain Bayes theorem and its use in updating probability distributions to incorporate new evidence, and use it to compute probabilities. 30. Diagram and explain the Naive Bayesian classification method. 31. Compute probabilities in small data sets and use these values in a naive Bayesian classifier to classify data items.","title":"csc522 MidTerm Review"},{"location":"2019-Fall/midTerm/#1-list-explain-and-identify-the-different-types-of-data-attributes-nominal-ordinal-interval-ratio","text":"Nominal: ==, != Ordinal: ==, !=, <, >, <=, >= Interval: ==, !=, <, >, <=, >=, +, - Ratio: ==, !=, <, >, <=, >=, +, -, *, /","title":"1. List, explain, and identify the different types of data attributes (nominal, ordinal, interval, ratio)."},{"location":"2019-Fall/midTerm/#2-explain-the-difference-between-supervised-and-unsupervised-learning","text":"Supervised: feature X with Label Y to predict(Regression, Classification). Unsupervised: feature X to learn itself to describe(Density estimation, Clustering, Dimensionality reduction).","title":"2. Explain the difference between supervised and unsupervised learning."},{"location":"2019-Fall/midTerm/#3-explain-the-nature-and-significance-of-noise-and-outliers-in-data-analysis","text":"Noise refers to modification of original values. Outliers are data objects with characteristics that are considerably different than most of the other data objects in the data set.","title":"3. Explain the nature and significance of noise and outliers in data analysis."},{"location":"2019-Fall/midTerm/#4-list-explain-compare-and-evaluate-methods-for-handling-missing-data-values","text":"Eliminate Data Objects: straightforward to implement, but signi cantly limits the scope and power of study, and introduces bias when data is not missing at random. Estimate Missing Values: Comparing to complete-case analysis, it can utilize all present data for analysis. However, some data imputation methods are only suitable when missing rate is low, e.g. mean- or median- lling and knn. Ignore the Missing Value During Analysis. Replace with all possible values(weighted by probabilities)","title":"4. List, explain, compare, and evaluate methods for handling missing data values."},{"location":"2019-Fall/midTerm/#5-explain-the-notion-of-sampling-and-list-and-explain-potential-problems-arising-in-sampling-data-sets","text":"Sampling is the use of a subset of the population to represent the whole population. Two key principles for effective sampling: - using a sample will work almost as well as using the entire data sets, if the sample is representative. A sample is representative if it has approximately the same property as the original set of data.","title":"5. Explain the notion of sampling, and list and explain potential problems arising in sampling data sets;"},{"location":"2019-Fall/midTerm/#6-explain-and-understand-how-principle-component-analysispca-works-and-can-be-applied-to-do-feature-reduction","text":"When applying PCA, the resulted principle components are always orthogonal(perpendicular) to one another. When applying PCA, each of principal components is always a linear combination of the original features.","title":"6. Explain and understand how Principle Component Analysis(PCA) works and can be applied to do feature reduction;"},{"location":"2019-Fall/midTerm/#7-list-explain-and-compare-different-methods-for-converting-continuous-attributes-into-discrete-attributes","text":"Discretization Attributes Transformation","title":"7. List, explain, and compare different methods for converting continuous attributes into discrete attributes."},{"location":"2019-Fall/midTerm/#8-explain-and-compute-the-mean-median-modes-and-z-scores-of-sets-of-data-values","text":"Frequency(\u9891\u7387\u6216\u9891\u6570): Mean(\u5e73\u5747\u6570): very sensitive to outliers Median(\u4e2d\u4f4d\u6570): Summary of frequency distribution Modes(\u4f17\u6570): Z-scores: the standard score is the signed fractional number of standard deviations by which the value of an observation or data point is above the mean value of what is being observed or measured. $$Z-scores = (oneData - mean) / standardDeviation$$","title":"8. Explain and compute the mean, median, modes, and Z-scores of sets of data values."},{"location":"2019-Fall/midTerm/#9-explain-the-notion-of-probability-distribution-and-compute-simple-distributions-from-data-frequencies","text":"","title":"9. Explain the notion of probability distribution and compute simple distributions from data frequencies."},{"location":"2019-Fall/midTerm/#10-explain-the-notion-of-conditional-probability-and-compute-conditional-probabilities-from-probability-distributions","text":"","title":"10. Explain the notion of conditional probability and compute conditional probabilities from probability distributions."},{"location":"2019-Fall/midTerm/#11-explain-the-notion-of-correlation-of-data-attributes","text":"Correlation measures the linear relationship between objects. Variance \u2013 measure of the deviation from the mean for points in one dimension. Covariance \u2013 a measure of how much each of the dimensions varies from the mean with respect to each other. $$Correlation(X, Y) = \\frac{Covariance(X, Y)}{Variance(X)Variance(Y)}$$","title":"11. Explain the notion of correlation of data attributes."},{"location":"2019-Fall/midTerm/#12-explain-and-give-examples-of-the-notion-of-decision-trees","text":"Given a collection of records (training set ): Each record contains a set of attributes, one of the attributes is the class. (Categorical, discrete, unordered) Find a model for class attribute as a function of the values of other attributes. Goal: previously unseen records (test set) should be assigned a class as accurately as possible.","title":"12. Explain and give examples of the notion of decision trees."},{"location":"2019-Fall/midTerm/#13-construct-decision-trees-from-small-data-sets-by-using-entropy-and-information-gain","text":"","title":"13. Construct decision trees from small data sets by using entropy and information gain."},{"location":"2019-Fall/midTerm/#14-compare-alternative-splitting-attributes-in-decision-tree-construction-by-applying-gini-or-entropy-measures","text":"","title":"14. Compare alternative splitting attributes in decision tree construction by applying gini or entropy measures."},{"location":"2019-Fall/midTerm/#15-explain-the-problems-caused-by-underfitting-and-overfitting-data-and-by-inexpressive-representations","text":"Underfitting: when model is too simple, both training and test errors are large. Overfitting: when model is complex where training error is small but test error is large.","title":"15. Explain the problems caused by underfitting and overfitting data, and by inexpressive representations."},{"location":"2019-Fall/midTerm/#16-understand-and-explain-generalization-error-optimistic-error-pessimistic-error-postpruning-based-on-optimistic-error-pessimistic-error","text":"Generalization error: Generalization error is the true error for the population of examples we would like to optimize. Optimistic error: Pessimistic:","title":"16. Understand and explain: generalization error, optimistic error, pessimistic error; postpruning based on optimistic error, pessimistic error."},{"location":"2019-Fall/midTerm/#17-how-to-build-decision-trees-with-missing-values-and-apply-the-decision-tree-to-test-data-with-missing-values","text":"","title":"17. How to build decision trees with missing values and apply the decision tree to test data with missing values."},{"location":"2019-Fall/midTerm/#18-chi-square-tests","text":"$$X^2 = \\sum{\\frac{O_i - E_i}{E_i}}$$ O is pratical value. E is theoretical value.","title":"18. Chi-square Tests"},{"location":"2019-Fall/midTerm/#19-explain-the-procedure-of-hold-out-stratified-sampling-cross-validation-and-loocv","text":"Holdout: 2/3 for training and 1/3 for testing. Stratified Sampling: sample in such a way that each class is represented in both sets. Cross-Validation LOOCV","title":"19. Explain the procedure of Hold-out, Stratified Sampling, Cross-Validation, and LOOCV."},{"location":"2019-Fall/midTerm/#20-contrast-and-explain-the-notions-of-error-rate-and-confusion-matrices-in-classification","text":"","title":"20. Contrast and explain the notions of error rate and confusion matrices in classification."},{"location":"2019-Fall/midTerm/#21-use-confusion-and-cost-matrices-to-compute-which-of-two-classifiers-is-better-for-a-data-set","text":"","title":"21. Use confusion and cost matrices to compute which of two classifiers is better for a data set."},{"location":"2019-Fall/midTerm/#22-explain-the-notion-of-accuracy-error-rate-precision-recall-f-measure","text":"$$Accuracy = \\frac{TP+TN}{TP+FP+TN+FN}$$ $$Error Rate = 1 - Accuracy$$ $$Precision(p) = \\frac{TP}{TP+FP}$$ $$Recall(r) = \\frac{TP}{TP+FN}$$ $$F-Measure(F) = \\frac{2rp}{r+p} = \\frac{2TP}{2TP+FP+FN}$$ The Accuracy (or error rate=1-Accuracy) is an inadequate measure of the performance of an algorithm, it doesn\u2019t take into account the cost of making wrong decisions.","title":"22. Explain the notion of Accuracy, Error Rate, Precision, Recall, F-measure."},{"location":"2019-Fall/midTerm/#23-explain-the-notion-of-an-roc-curve-auc-and-its-meaning-for-classifier-performance","text":"","title":"23. Explain the notion of an ROC curve, AUC, and its meaning for classifier performance."},{"location":"2019-Fall/midTerm/#24-use-roc-curves-to-compare-performance-of-different-classifiers","text":"","title":"24. Use ROC curves to compare performance of different classifiers."},{"location":"2019-Fall/midTerm/#25-understanding-the-procedure-of-bagging-and-boosting-especially-the-adaboost","text":"","title":"25. Understanding the procedure of Bagging and Boosting, especially the Adaboost."},{"location":"2019-Fall/midTerm/#26-explain-and-understand-how-the-bagging-and-adaboost-works","text":"","title":"26. Explain and understand how the bagging and AdaBoost works."},{"location":"2019-Fall/midTerm/#27-explain-how-the-k-nearest-neighbour-classifier-works","text":"","title":"27. Explain how the K-nearest Neighbour classifier works."},{"location":"2019-Fall/midTerm/#28-explain-the-notion-of-probabilistic-or-bayesian-methods","text":"","title":"28. Explain the notion of probabilistic or Bayesian methods."},{"location":"2019-Fall/midTerm/#29-state-and-explain-bayes-theorem-and-its-use-in-updating-probability-distributions-to-incorporate-new-evidence-and-use-it-to-compute-probabilities","text":"","title":"29. State and explain Bayes theorem and its use in updating probability distributions to incorporate new evidence, and use it to compute probabilities."},{"location":"2019-Fall/midTerm/#30-diagram-and-explain-the-naive-bayesian-classification-method","text":"","title":"30. Diagram and explain the Naive Bayesian classification method."},{"location":"2019-Fall/midTerm/#31-compute-probabilities-in-small-data-sets-and-use-these-values-in-a-naive-bayesian-classifier-to-classify-data-items","text":"","title":"31. Compute probabilities in small data sets and use these values in a naive Bayesian classifier to classify data items."},{"location":"csc568/0-index/","text":"Number 22 is a review class, in which it talks about some examples in NetApp.","title":"Preface"},{"location":"csc568/10-SAN/","text":"SAN(Storage Area Network) Organize connection between storage and servers . SCSI SCSI is a kind of block protocol. Others are files or objects. iSCSI iSCSI, FCOE fiber channel over Ethernet substitution of fiber channel transport protocol(block protocol) SCSI, HiPPI, ESCO switched fabric block I/O // to do ![SANarchictecture][] HBA Host Bus Adapter: cable, hub, switch About SAN Channel Oriented channel processor Network Oriented full multiplexing peer to peer internetwork Benefits of SAN high bandwidth SCSI extension resource consolidation centralized storage, management security(it's the server initiates the I/O) scalability","title":"10 - SAN"},{"location":"csc568/10-SAN/#sanstorage-area-network","text":"Organize connection between storage and servers .","title":"SAN(Storage Area Network)"},{"location":"csc568/10-SAN/#scsi","text":"SCSI is a kind of block protocol. Others are files or objects.","title":"SCSI"},{"location":"csc568/10-SAN/#iscsi","text":"iSCSI, FCOE fiber channel over Ethernet substitution of fiber channel transport protocol(block protocol) SCSI, HiPPI, ESCO switched fabric block I/O // to do ![SANarchictecture][]","title":"iSCSI"},{"location":"csc568/10-SAN/#hba","text":"Host Bus Adapter: cable, hub, switch","title":"HBA"},{"location":"csc568/10-SAN/#about-san","text":"Channel Oriented channel processor Network Oriented full multiplexing peer to peer internetwork","title":"About SAN"},{"location":"csc568/10-SAN/#benefits-of-san","text":"high bandwidth SCSI extension resource consolidation centralized storage, management security(it's the server initiates the I/O) scalability","title":"Benefits of SAN"},{"location":"csc568/11-NAS/","text":"NAS(Network Attached Storage) File-based and client-server model. Protocol for NAS well-defined message and response support OS file operations Sketch client: initialize the request server: locate file, response the file through network, resolve conflict, prioritize design: what APIs are needed to finish the jobs? (straightforward protocol) how elaborate the client/server is? (server has to handle many clients at the same time) so the simpler server is, the more concurrency server can be. e.g. JavaScript. Client translate users' calls to NAS protocol cooperate with server heart beat message cache(latency-tolerance device) Server: manage data(efficiently, reliably, securely...) could prefetch could upcalls(communicate with clients). upcalls are limited on stateful protocol Protocols of NAS NFS stateless hierarchy no upcalls from servers SMB(CIFS) more \"talktive\" generic: printer","title":"11 - NAS"},{"location":"csc568/11-NAS/#nasnetwork-attached-storage","text":"File-based and client-server model.","title":"NAS(Network Attached Storage)"},{"location":"csc568/11-NAS/#protocol-for-nas","text":"well-defined message and response support OS file operations","title":"Protocol for NAS"},{"location":"csc568/11-NAS/#sketch","text":"client: initialize the request server: locate file, response the file through network, resolve conflict, prioritize design: what APIs are needed to finish the jobs? (straightforward protocol) how elaborate the client/server is? (server has to handle many clients at the same time) so the simpler server is, the more concurrency server can be. e.g. JavaScript.","title":"Sketch"},{"location":"csc568/11-NAS/#client","text":"translate users' calls to NAS protocol cooperate with server heart beat message cache(latency-tolerance device)","title":"Client"},{"location":"csc568/11-NAS/#server","text":"manage data(efficiently, reliably, securely...) could prefetch could upcalls(communicate with clients). upcalls are limited on stateful protocol","title":"Server:"},{"location":"csc568/11-NAS/#protocols-of-nas","text":"NFS stateless hierarchy no upcalls from servers SMB(CIFS) more \"talktive\" generic: printer","title":"Protocols of NAS"},{"location":"csc568/12-DFS/","text":"DFS(Distributed File System) How to compare different file systems? What's the measurements of DFS? 1. View hierarchical FS global view: all clients have the same view local view: client can have different views, mount 2. State open on clients also open on server(connection) server must remember client must close comparison(statefull vs stateless) message is longer(stateless) server is simpler idempotent(stateless) lock or lease stateful -> client locks a file stateless -> lease a period of time 3. Cache Where does the caching occur? clients: memory/disks server: memory Cache Size Choice: once it's determined, changing it is risky!(Cache background....any solution to it) Cache Replacement Policy: LRU, approximate policy - clock Consistency: replication, update Cache Consistency Policy: UNIX file consistency: no design on it, it leaves to the application to handle the consistency problem. e.g. Google doc. write-through-cache(relatively write-through-disk) dealyed-write: delay some files write-on-close: delay one single file delayed-write-on-close: the lifetime of files are usually short 4. Replication block-based access raw block: typically high performance applicaiton must provide all data manage leverage: e.g. ls file clustered file server file-based access Virtualization. Benefits over block-based access: abstraction hides the implementation server independence Fine-grained data management permission backup(e.g. S3) share data cost high CPU per request network technology: infi band RDMA: rget, rput","title":"12 - DFS"},{"location":"csc568/12-DFS/#dfsdistributed-file-system","text":"How to compare different file systems? What's the measurements of DFS?","title":"DFS(Distributed File System)"},{"location":"csc568/12-DFS/#1-view","text":"hierarchical FS global view: all clients have the same view local view: client can have different views, mount","title":"1. View"},{"location":"csc568/12-DFS/#2-state","text":"open on clients also open on server(connection) server must remember client must close comparison(statefull vs stateless) message is longer(stateless) server is simpler idempotent(stateless) lock or lease stateful -> client locks a file stateless -> lease a period of time","title":"2. State"},{"location":"csc568/12-DFS/#3-cache","text":"Where does the caching occur? clients: memory/disks server: memory Cache Size Choice: once it's determined, changing it is risky!(Cache background....any solution to it) Cache Replacement Policy: LRU, approximate policy - clock Consistency: replication, update Cache Consistency Policy: UNIX file consistency: no design on it, it leaves to the application to handle the consistency problem. e.g. Google doc. write-through-cache(relatively write-through-disk) dealyed-write: delay some files write-on-close: delay one single file delayed-write-on-close: the lifetime of files are usually short","title":"3. Cache"},{"location":"csc568/12-DFS/#4-replication","text":"","title":"4. Replication"},{"location":"csc568/12-DFS/#block-based-access","text":"raw block: typically high performance applicaiton must provide all data manage leverage: e.g. ls file clustered file server","title":"block-based access"},{"location":"csc568/12-DFS/#file-based-access","text":"Virtualization. Benefits over block-based access: abstraction hides the implementation server independence Fine-grained data management permission backup(e.g. S3) share data cost high CPU per request network technology: infi band RDMA: rget, rput","title":"file-based access"},{"location":"csc568/13-Ceph/","text":"Ceph A kind of distributed file system, using CRUSH algorithm . Goal scalability capacity throughtput client performance individual emphasis on HPO: shared files, lock-step manner(distribute jobs to processors then merge) reliability dynamic(peta byte) build incrementally failures quality and character of workload changes performance Key Ideas objects based storage system decouple data and metadata They are in different clusters They have different retreving pattern Archictecture Overveiw Key features Servers decouple data and metadata CRUSH: controlled repliction under scalable washing files striped onto predictable objects CRUSH maps objects to storage device Dynamically distribute meta data management metadata operations make up 50% of all operations dynamic subtree partitioning Object based storage Others: migrate, replication, failure detection, recovery Clients ceph interface POSIX compliant decouple data and metadata user space implementation example: client sends request to MDS MDS returns capability, file index, size, stripe information... client reads/writes directly from/to OSD MDS mnage the capability client sends close synchronization adhere to POSIX include HPC-oriented extensions consistency/correctness by defaults optimally relax constraint, like clients could write to different locations extension for both data and metadata synchronize I/O used on multiple writes, or mix reader and writer. distribute metadata HDS used journaling repetition metadata updates in memory optimize on-disk layout for read access adaptively distributed cached metadata accessed nodes distributed objected storage files are splited into objects objects are members of placement groups placement groups are distributed to OSD","title":"13 - Ceph"},{"location":"csc568/13-Ceph/#ceph","text":"A kind of distributed file system, using CRUSH algorithm .","title":"Ceph"},{"location":"csc568/13-Ceph/#goal","text":"scalability capacity throughtput client performance individual emphasis on HPO: shared files, lock-step manner(distribute jobs to processors then merge) reliability dynamic(peta byte) build incrementally failures quality and character of workload changes performance","title":"Goal"},{"location":"csc568/13-Ceph/#key-ideas","text":"objects based storage system decouple data and metadata They are in different clusters They have different retreving pattern","title":"Key Ideas"},{"location":"csc568/13-Ceph/#archictecture-overveiw","text":"","title":"Archictecture Overveiw"},{"location":"csc568/13-Ceph/#key-features","text":"","title":"Key features"},{"location":"csc568/13-Ceph/#servers","text":"decouple data and metadata CRUSH: controlled repliction under scalable washing files striped onto predictable objects CRUSH maps objects to storage device Dynamically distribute meta data management metadata operations make up 50% of all operations dynamic subtree partitioning Object based storage Others: migrate, replication, failure detection, recovery","title":"Servers"},{"location":"csc568/13-Ceph/#clients","text":"ceph interface POSIX compliant decouple data and metadata user space implementation example: client sends request to MDS MDS returns capability, file index, size, stripe information... client reads/writes directly from/to OSD MDS mnage the capability client sends close synchronization adhere to POSIX include HPC-oriented extensions consistency/correctness by defaults optimally relax constraint, like clients could write to different locations extension for both data and metadata synchronize I/O used on multiple writes, or mix reader and writer.","title":"Clients"},{"location":"csc568/13-Ceph/#distribute-metadata","text":"HDS used journaling repetition metadata updates in memory optimize on-disk layout for read access adaptively distributed cached metadata accessed nodes distributed objected storage files are splited into objects objects are members of placement groups placement groups are distributed to OSD","title":"distribute metadata"},{"location":"csc568/14-GFS/","text":"GFS(Google File System) What does it do? set out to build a DFS willing to change anything(e.g. client API: gfs-open, gfs-read...) apps modification/cooperation Design constraints compliant to failure in norm bugs, human errors, power loss maintaining, detecting and recovery files are huge multi-GB are common. Comparing to billions of KB files most modification are appends random writes are practically non-exist many files are written once and read many times sequentially two types of reads large streaming read small random reads google gathers websites and build inddddd indice. skip and forward, always in forwad dimention. sustained bandwidth 7 latency Architectual Design GFS cluster a single master multiple chunk servers chunk servers can be accessed by clients compliant linux servers File represent as fix-sized chunk(like object in Ceph) 64-bits unique ID stored at chunk server(deliver directly from chunk server, reducing throughput latency) 3 way replication Master metadata management Clients master the metadata data from chunk servers it doesn't use VFS no caching at clients. I/O are usually streaming, no temporary cache ..... Archictecture Overveiw Why Single Master Design? simple. Multi-masters have to cooperate or replicate master. master only ensure of chunk locations clients typically ask for multiple chunk locations in one request chunk size 64MB or 64bits(ID) fewer requests to master fewer metadata entries fragmentation(64MB per chunk) 64bits per 64MB chunk, cache chunk ID in memory chunk location no persistent: clients just know the chunks but not their locations startup: send info to master operation logs metadata updates are logs take global snapshots to truncate logs Lease 60s time-out renew indefinitely","title":"14 - GFS"},{"location":"csc568/14-GFS/#gfsgoogle-file-system","text":"","title":"GFS(Google File System)"},{"location":"csc568/14-GFS/#what-does-it-do","text":"set out to build a DFS willing to change anything(e.g. client API: gfs-open, gfs-read...) apps modification/cooperation","title":"What does it do?"},{"location":"csc568/14-GFS/#design-constraints","text":"compliant to failure in norm bugs, human errors, power loss maintaining, detecting and recovery files are huge multi-GB are common. Comparing to billions of KB files most modification are appends random writes are practically non-exist many files are written once and read many times sequentially two types of reads large streaming read small random reads google gathers websites and build inddddd indice. skip and forward, always in forwad dimention. sustained bandwidth 7 latency","title":"Design constraints"},{"location":"csc568/14-GFS/#architectual-design","text":"GFS cluster a single master multiple chunk servers chunk servers can be accessed by clients compliant linux servers File represent as fix-sized chunk(like object in Ceph) 64-bits unique ID stored at chunk server(deliver directly from chunk server, reducing throughput latency) 3 way replication Master metadata management Clients master the metadata data from chunk servers it doesn't use VFS no caching at clients. I/O are usually streaming, no temporary cache .....","title":"Architectual Design"},{"location":"csc568/14-GFS/#archictecture-overveiw","text":"","title":"Archictecture Overveiw"},{"location":"csc568/14-GFS/#why-single-master-design","text":"simple. Multi-masters have to cooperate or replicate master. master only ensure of chunk locations clients typically ask for multiple chunk locations in one request chunk size 64MB or 64bits(ID) fewer requests to master fewer metadata entries fragmentation(64MB per chunk) 64bits per 64MB chunk, cache chunk ID in memory chunk location no persistent: clients just know the chunks but not their locations startup: send info to master operation logs metadata updates are logs take global snapshots to truncate logs","title":"Why Single Master Design?"},{"location":"csc568/14-GFS/#lease","text":"60s time-out renew indefinitely","title":"Lease"},{"location":"csc568/15-LFS/","text":"Log FS Objectives optimize to the common case understand the program clean, simple solution Why LFS read will be satified by cache disk accesses are mostly writes Benefits of LFS faster write performance(similar read performance) faster crash recovery(worse than fsck) Existing System 4 observations processor speed -> grow up disk seek -> not improve as fast main memory and cache size -> grow up Max kernels processor -> grow up Two general problems information is spread all servers: many small accesses e.g. 5 small IOs to create a file synchronize writes Bundles writes - high write bandwidth How - delay write - write large continuous extents Circular log - threading - copy compacting setment, level of inherizection Segment Cleaning Issues - when - how many to change - which segment - how to copy WAFL_LFS","title":"15 - LFS"},{"location":"csc568/15-LFS/#log-fs","text":"","title":"Log FS"},{"location":"csc568/15-LFS/#objectives","text":"optimize to the common case understand the program clean, simple solution","title":"Objectives"},{"location":"csc568/15-LFS/#why-lfs","text":"read will be satified by cache disk accesses are mostly writes","title":"Why LFS"},{"location":"csc568/15-LFS/#benefits-of-lfs","text":"faster write performance(similar read performance) faster crash recovery(worse than fsck)","title":"Benefits of LFS"},{"location":"csc568/15-LFS/#existing-system","text":"","title":"Existing System"},{"location":"csc568/15-LFS/#4-observations","text":"processor speed -> grow up disk seek -> not improve as fast main memory and cache size -> grow up Max kernels processor -> grow up","title":"4 observations"},{"location":"csc568/15-LFS/#two-general-problems","text":"information is spread all servers: many small accesses e.g. 5 small IOs to create a file synchronize writes Bundles writes - high write bandwidth How - delay write - write large continuous extents Circular log - threading - copy compacting setment, level of inherizection Segment Cleaning Issues - when - how many to change - which segment - how to copy","title":"Two general problems"},{"location":"csc568/15-LFS/#wafl_lfs","text":"","title":"WAFL_LFS"},{"location":"csc568/16-Redundancy/","text":"Redundancy Resiliency in Storage replication: erasure encoding(storage cost) parity(computation cost) EC super set and both(parity and replication) form of FEC(Forward Error Correction) msg length(k) code word length(m) n = m + k code rate = k/n Voting send bit 3 times 8 cases, accept the majority Reed-Solomon codes family of codes old RAID parity n = k + 1 N(wordsize) = 1 RAID 6 n = k + 2 N = 1 Erasure encoding k disks & data n disks m disks of code, n = k + m w-bit words Assume 1 w-bit word/disk $$data = d_0, d_1, ... d_{k-1}$$ $$code = c_0, c_1, ... c_{m-1}$$ $$c_0 = a_{0, 0}d_0 + ... + a_{0, k-1}d_{k-1}$$ $$...$$ $$c_{m-1} = a_{m-1, 0} + ... + a_{m-1, k-1}d_{k-1}$$ Example Parity: \"+\" - XOR; \"*\" - AND; with 1. Max distance separable MDS reconstruct from any m failures Longer W richer set of erasure codes must define arithmetic positive numbers overflow Galois Field/Finite Field Linux RAID 6 (W = 8) $$c_0 = a_{0, 0}d_0 + a_{0, 1}d_1$$ $$c_1 = a_{1, 0}d_0 + a_{1, 1}d_1$$ $$p = d_0~xor~d_1$$ Diagonal. Use multiple disks to recover In storage system, bits are lost rather than flipped. Python lib: rs RSCoder(h, m) encode and decode RS: Reed-Solomon","title":"16 - Redundancy"},{"location":"csc568/16-Redundancy/#redundancy","text":"","title":"Redundancy"},{"location":"csc568/16-Redundancy/#resiliency-in-storage","text":"replication: erasure encoding(storage cost) parity(computation cost)","title":"Resiliency in Storage"},{"location":"csc568/16-Redundancy/#ec","text":"super set and both(parity and replication) form of FEC(Forward Error Correction) msg length(k) code word length(m) n = m + k code rate = k/n","title":"EC"},{"location":"csc568/16-Redundancy/#voting","text":"send bit 3 times 8 cases, accept the majority","title":"Voting"},{"location":"csc568/16-Redundancy/#reed-solomon-codes","text":"family of codes old","title":"Reed-Solomon codes"},{"location":"csc568/16-Redundancy/#raid-parity","text":"n = k + 1 N(wordsize) = 1 RAID 6 n = k + 2 N = 1","title":"RAID parity"},{"location":"csc568/16-Redundancy/#erasure-encoding","text":"k disks & data n disks m disks of code, n = k + m w-bit words Assume 1 w-bit word/disk $$data = d_0, d_1, ... d_{k-1}$$ $$code = c_0, c_1, ... c_{m-1}$$ $$c_0 = a_{0, 0}d_0 + ... + a_{0, k-1}d_{k-1}$$ $$...$$ $$c_{m-1} = a_{m-1, 0} + ... + a_{m-1, k-1}d_{k-1}$$","title":"Erasure encoding"},{"location":"csc568/16-Redundancy/#example","text":"Parity: \"+\" - XOR; \"*\" - AND; with 1. Max distance separable MDS reconstruct from any m failures Longer W richer set of erasure codes must define arithmetic positive numbers overflow Galois Field/Finite Field Linux RAID 6 (W = 8) $$c_0 = a_{0, 0}d_0 + a_{0, 1}d_1$$ $$c_1 = a_{1, 0}d_0 + a_{1, 1}d_1$$ $$p = d_0~xor~d_1$$ Diagonal. Use multiple disks to recover In storage system, bits are lost rather than flipped.","title":"Example"},{"location":"csc568/16-Redundancy/#python-lib-rs","text":"RSCoder(h, m) encode and decode RS: Reed-Solomon","title":"Python lib: rs"},{"location":"csc568/17-Deduplication/","text":"Data Deduplication Working on slow networks Make local coppies: must worry about update conflicts Use remote login: only for text-based applications Use instead a LBFS: better than remote login; must deal with issues like auto-saves blocking the editor for the duration of transfer. LBFS LBFS file server divides the files it stores into chunks and indexes the chunks by hash value LBFS client similarly indexes a large persistent file cache LBFS never transfers chunks that recipient already has Tradeoff A: space (index) for time (bandwidth) Tradeoff B: meta info for data All transfers are compressed Tradeoff: processing for bandwidth LBFS Read and Write Other issues Protocol File Consistency Security Implementation Conclusion Under normal circumstances, LBFS consumes 90% less bandwidth than traditional file systems. Makes transparent remote file access a viable and less frustrating alternative to running interactive programs on remote machines. Data Deduplication Advantages: Less disk, Less bandwidth, Less power Disadvantages: More time (compute), More complex, Latent channels Eager method: Dedup on write Lazy method: Dedup periodically Fixed: Index/offset determines the block boundary Variable: The data itself determines the block boundary Tradeoffs?","title":"17 - Deduplication"},{"location":"csc568/17-Deduplication/#data-deduplication","text":"","title":"Data Deduplication"},{"location":"csc568/17-Deduplication/#working-on-slow-networks","text":"Make local coppies: must worry about update conflicts Use remote login: only for text-based applications Use instead a LBFS: better than remote login; must deal with issues like auto-saves blocking the editor for the duration of transfer.","title":"Working on slow networks"},{"location":"csc568/17-Deduplication/#lbfs","text":"LBFS file server divides the files it stores into chunks and indexes the chunks by hash value LBFS client similarly indexes a large persistent file cache LBFS never transfers chunks that recipient already has Tradeoff A: space (index) for time (bandwidth) Tradeoff B: meta info for data All transfers are compressed Tradeoff: processing for bandwidth","title":"LBFS"},{"location":"csc568/17-Deduplication/#lbfs-read-and-write","text":"","title":"LBFS Read and Write"},{"location":"csc568/17-Deduplication/#other-issues","text":"Protocol File Consistency Security Implementation","title":"Other issues"},{"location":"csc568/17-Deduplication/#conclusion","text":"Under normal circumstances, LBFS consumes 90% less bandwidth than traditional file systems. Makes transparent remote file access a viable and less frustrating alternative to running interactive programs on remote machines.","title":"Conclusion"},{"location":"csc568/17-Deduplication/#data-deduplication_1","text":"Advantages: Less disk, Less bandwidth, Less power Disadvantages: More time (compute), More complex, Latent channels Eager method: Dedup on write Lazy method: Dedup periodically Fixed: Index/offset determines the block boundary Variable: The data itself determines the block boundary Tradeoffs?","title":"Data Deduplication"},{"location":"csc568/18-GNR/","text":"General Parallel File System Native RAID Preview Why? Hard disk rates are lagging Challenge: how to design a reliable HPC storage system using 100K+ disk drives? Background: GPFS, Parallel Computing, RAID Traditonal Storage Stack Problems with Traditional RAID and Disks Performance: Traditional RAID rebuild significantly affects performance. With 100,000 disks, disk drive failures are expected to happen on a daily basis. Disks are getting bigger and hence, take longer to rebuild \"Silent\" data corruption in disk drives Solution Why Native RAID? Higher Performance Use declustered RAID to minimize performance degradation during rebuild Extreme data integrity Use end-to-end checksums and version numbers to detect, locate and correct silent disk corruption Summary","title":"18 - GNR"},{"location":"csc568/18-GNR/#general-parallel-file-system-native-raid","text":"","title":"General Parallel File System Native RAID"},{"location":"csc568/18-GNR/#preview","text":"Why? Hard disk rates are lagging Challenge: how to design a reliable HPC storage system using 100K+ disk drives? Background: GPFS, Parallel Computing, RAID","title":"Preview"},{"location":"csc568/18-GNR/#traditonal-storage-stack","text":"","title":"Traditonal Storage Stack"},{"location":"csc568/18-GNR/#problems-with-traditional-raid-and-disks","text":"Performance: Traditional RAID rebuild significantly affects performance. With 100,000 disks, disk drive failures are expected to happen on a daily basis. Disks are getting bigger and hence, take longer to rebuild \"Silent\" data corruption in disk drives","title":"Problems with Traditional RAID and Disks"},{"location":"csc568/18-GNR/#solution","text":"","title":"Solution"},{"location":"csc568/18-GNR/#why-native-raid","text":"Higher Performance Use declustered RAID to minimize performance degradation during rebuild Extreme data integrity Use end-to-end checksums and version numbers to detect, locate and correct silent disk corruption","title":"Why Native RAID?"},{"location":"csc568/18-GNR/#summary","text":"","title":"Summary"},{"location":"csc568/19-Erasure-Encoding/","text":"Ensure Encoding","title":"19 - Erasure Encoding"},{"location":"csc568/19-Erasure-Encoding/#ensure-encoding","text":"","title":"Ensure Encoding"},{"location":"csc568/20-Business-Continuity/","text":"Business Continuity Outage preparation for response to recovery from Solution to address unavailability degrading problem ---------RPO(Recovery point objective)-----||outaget||------RTO(recovery time objective) make RPO/RTO as close to the outage as possible recover vs restart Recovery: restoring preve copy of data, backup Restart: restoring mirrored consistent copy, online spare Dentify single point failure network power Disk/RAID failure site HBA Node Backup Backup is an additional data that can be used for restoring, and may not be the only purpose. Backup is used only if the primary is lost or not available. Retention spentimental archiving(regulation) kinds: full, accummulative, incrementals hot/cold backup / snapshot Backup and Restore Backup Restore Full O(n) O(n) Accummulative O(nt) O(n)+O(acc) Incrementals O(inc) O(n)+T*O(inc)","title":"20 - Business Continuity"},{"location":"csc568/20-Business-Continuity/#business-continuity","text":"","title":"Business Continuity"},{"location":"csc568/20-Business-Continuity/#outage","text":"preparation for response to recovery from","title":"Outage"},{"location":"csc568/20-Business-Continuity/#solution-to-address","text":"unavailability degrading problem ---------RPO(Recovery point objective)-----||outaget||------RTO(recovery time objective) make RPO/RTO as close to the outage as possible","title":"Solution to address"},{"location":"csc568/20-Business-Continuity/#recover-vs-restart","text":"Recovery: restoring preve copy of data, backup Restart: restoring mirrored consistent copy, online spare","title":"recover vs restart"},{"location":"csc568/20-Business-Continuity/#dentify-single-point-failure","text":"network power Disk/RAID failure site HBA Node","title":"Dentify single point failure"},{"location":"csc568/20-Business-Continuity/#backup","text":"Backup is an additional data that can be used for restoring, and may not be the only purpose. Backup is used only if the primary is lost or not available.","title":"Backup"},{"location":"csc568/20-Business-Continuity/#retention","text":"spentimental archiving(regulation) kinds: full, accummulative, incrementals hot/cold backup / snapshot","title":"Retention"},{"location":"csc568/20-Business-Continuity/#backup-and-restore","text":"Backup Restore Full O(n) O(n) Accummulative O(nt) O(n)+O(acc) Incrementals O(inc) O(n)+T*O(inc)","title":"Backup and Restore"},{"location":"csc568/21-SCM/","text":"Storage Class Memory Overview DRAM vs SCM similar latency cost: SCM << DRAM / bit SCM is denser than DRAM size: |SCM| >> |DRAM| Technologies Flash: NAD, NGD, 3D PCM magnetoresistive: RAM persistent RAM Why don't use flash for SCM? No consistent semantics for writing a page, a word or a block. Address for data may change along with the program. wear-out problem. Persistent Memory vs Volatile Memory Persistent Memory files open/close/... names(string -> file) read/write Volatile Memory malloc/free(heap) call/return(stack) global(static) no name but logic address load/storage","title":"21 - SCM"},{"location":"csc568/21-SCM/#storage-class-memory","text":"","title":"Storage Class Memory"},{"location":"csc568/21-SCM/#overview","text":"","title":"Overview"},{"location":"csc568/21-SCM/#dram-vs-scm","text":"similar latency cost: SCM << DRAM / bit SCM is denser than DRAM size: |SCM| >> |DRAM|","title":"DRAM vs SCM"},{"location":"csc568/21-SCM/#technologies","text":"Flash: NAD, NGD, 3D PCM magnetoresistive: RAM persistent RAM Why don't use flash for SCM? No consistent semantics for writing a page, a word or a block. Address for data may change along with the program. wear-out problem.","title":"Technologies"},{"location":"csc568/21-SCM/#persistent-memory-vs-volatile-memory","text":"Persistent Memory files open/close/... names(string -> file) read/write Volatile Memory malloc/free(heap) call/return(stack) global(static) no name but logic address load/storage","title":"Persistent Memory vs Volatile Memory"},{"location":"csc568/22-NetApp-case/","text":"","title":"22 NetApp case"},{"location":"csc568/23-Haystack/","text":"Haystack Why Haystack? Find a needle in the haystack. Facebook storage. photo at FB(2010): 260 Billion images; 20PB storage; 60TB upload/week feature: long-tail access usage of CDN: if file is not in CDN, CDN asks for photo from servers. NFS: a photo is a file too much metadata most meta data are not used bigger problem: meta data has to be read from disk disk IOs for metadata is a limiting factor solutions: keep metadata in memory cache: has limited impact on the number of IOs. storage system has a long tail(will miss in CDN). url generated by haystack directory: http:// / / / . if it's not in cdn, go to haystack cache to get the file. Haystack store: persistent capacity(is divided into physical volumes) e.g. 100TB in server -> 100 * 100GB volumes logical volume group of physical volume, distributed across different machines Directory: maintain metadata maintain free space 4 functions: logical -> physical mapping load balance writes determine whether to fetch from CDN or cache identify read-only store machines add new machines: increase capacity write-enabled over time sapce is exhausted, then mark as read-only. In memory metadata logical volume id for photo offset size basic and simple read makes every specific request each volume store millions of photos a physical volume is one file(haystack - logical volume id) access the photo: open once the file, cache inode in the memory, then lseed for each photo Haystack Cache: internal CDN receive requests from broswer/CDN it caches the requests come from the browser photo is fetched from a write-enabled store: shelter store write-enabled store from read accesses photos are most heavily accessed just after upload fs generally perform better when doing reads or writes only. Implementation","title":"23 - Haystack"},{"location":"csc568/23-Haystack/#haystack","text":"","title":"Haystack"},{"location":"csc568/23-Haystack/#why-haystack","text":"Find a needle in the haystack. Facebook storage. photo at FB(2010): 260 Billion images; 20PB storage; 60TB upload/week feature: long-tail access usage of CDN: if file is not in CDN, CDN asks for photo from servers. NFS: a photo is a file too much metadata most meta data are not used bigger problem: meta data has to be read from disk disk IOs for metadata is a limiting factor solutions: keep metadata in memory cache: has limited impact on the number of IOs. storage system has a long tail(will miss in CDN). url generated by haystack directory: http:// / / / . if it's not in cdn, go to haystack cache to get the file.","title":"Why Haystack?"},{"location":"csc568/23-Haystack/#haystack-store","text":"persistent capacity(is divided into physical volumes) e.g. 100TB in server -> 100 * 100GB volumes logical volume group of physical volume, distributed across different machines","title":"Haystack store:"},{"location":"csc568/23-Haystack/#directory","text":"maintain metadata maintain free space 4 functions: logical -> physical mapping load balance writes determine whether to fetch from CDN or cache identify read-only store machines add new machines: increase capacity write-enabled over time sapce is exhausted, then mark as read-only.","title":"Directory:"},{"location":"csc568/23-Haystack/#in-memory-metadata","text":"logical volume id for photo offset size basic and simple read makes every specific request each volume store millions of photos a physical volume is one file(haystack - logical volume id) access the photo: open once the file, cache inode in the memory, then lseed for each photo","title":"In memory metadata"},{"location":"csc568/23-Haystack/#haystack-cache-internal-cdn","text":"receive requests from broswer/CDN it caches the requests come from the browser photo is fetched from a write-enabled store: shelter store write-enabled store from read accesses photos are most heavily accessed just after upload fs generally perform better when doing reads or writes only.","title":"Haystack Cache: internal CDN"},{"location":"csc568/23-Haystack/#implementation","text":"","title":"Implementation"},{"location":"csc568/24-Disasters/","text":"Designing for Disasters Disasters availability data loss How reliability primary and secondary isolated from failure availability fail over reconstruction Design Rapid recovery vs Minimum data loss Recovery Backup for reliability Remote Mirroring asynchronous batch write-preserving synchronous(low RTO)","title":"24 - Disasters"},{"location":"csc568/24-Disasters/#designing-for-disasters","text":"","title":"Designing for Disasters"},{"location":"csc568/24-Disasters/#disasters","text":"availability data loss","title":"Disasters"},{"location":"csc568/24-Disasters/#how","text":"reliability primary and secondary isolated from failure availability fail over reconstruction","title":"How"},{"location":"csc568/24-Disasters/#design","text":"Rapid recovery vs Minimum data loss Recovery Backup for reliability Remote Mirroring asynchronous batch write-preserving synchronous(low RTO)","title":"Design"},{"location":"csc568/25-DevFS/","text":"Design Direct Access FS New Storage Steak required existing OS trap in OS layers: buffer, VFS, device drivers lots of overhead spend millions of cycles File System Architecture control planes permission consistency sharing data planes rw data rw metadata kernel FS manage both control and data trap into OS lib FS all ops on Application hard to persist and share Hybrid FS(lib + kernel) common data and metadata, ops no trap kernel permission check share Hybrid FS + trusted server micro-kernel like approach trusted control process facilitates, control plane ops eliminte traps Direct access FS Challenges lib FS fails to satisfy 3 important properties: integrity. kernel FS manages in-memory and on-disks data. single process concurrency: crash concurrency, permission enforcement Benefits can exploit the parallelism fo the hardware: multiple IO channels on CPU devFS can directly control volitile memory helps the creash recovery Limitations increase the memroy footprint dev CPU are slower(to save power) are less capable(not have some features as ordinary CPU) less features(no backup, checksum, snapshot) separate devFS from environment Design Principles Disentangle FS structure to embrace dev-level parallelism Guarantee FS integrity without compromising direct access Simplify crash recovery Reduce dev memory footprint to FS Enable minimal OS-level state share with dev FS Design move FS into device Hardware use device-level CPU and memory apps bypass OS for control and data planes Device handles: integrity, crash and recovery, security","title":"25 - DevFS"},{"location":"csc568/25-DevFS/#design-direct-access-fs","text":"","title":"Design Direct Access FS"},{"location":"csc568/25-DevFS/#new-storage-steak","text":"required existing OS trap in OS layers: buffer, VFS, device drivers lots of overhead spend millions of cycles","title":"New Storage Steak"},{"location":"csc568/25-DevFS/#file-system-architecture","text":"control planes permission consistency sharing data planes rw data rw metadata kernel FS manage both control and data trap into OS lib FS all ops on Application hard to persist and share Hybrid FS(lib + kernel) common data and metadata, ops no trap kernel permission check share Hybrid FS + trusted server micro-kernel like approach trusted control process facilitates, control plane ops eliminte traps Direct access FS Challenges lib FS fails to satisfy 3 important properties: integrity. kernel FS manages in-memory and on-disks data. single process concurrency: crash concurrency, permission enforcement Benefits can exploit the parallelism fo the hardware: multiple IO channels on CPU devFS can directly control volitile memory helps the creash recovery Limitations increase the memroy footprint dev CPU are slower(to save power) are less capable(not have some features as ordinary CPU) less features(no backup, checksum, snapshot) separate devFS from environment","title":"File System Architecture"},{"location":"csc568/25-DevFS/#design-principles","text":"Disentangle FS structure to embrace dev-level parallelism Guarantee FS integrity without compromising direct access Simplify crash recovery Reduce dev memory footprint to FS Enable minimal OS-level state share with dev FS","title":"Design Principles"},{"location":"csc568/25-DevFS/#design","text":"move FS into device Hardware use device-level CPU and memory apps bypass OS for control and data planes Device handles: integrity, crash and recovery, security","title":"Design"},{"location":"csc568/26-SSD-Reliability/","text":"Design Tradeoffs for SSD Reliability High-level objectives Understand the SSD-internal mechanisms behind fail-slow symptoms Examine SSD-internal reliability enhancement techniques Think about system- and device-level approaches for handling errors SSD's reliability issue How to make SSD reliable? Performance overhead? Across different chips and wear states? Flash memory error modeling SSD reliability enhancements Error correction code predictable performance is fixed at design-time Data re-reads is much more powerful than ECC increases latency for correcting erros Intra-SSD redundancy Protects against random and sporadic errors increases write amplification increases read amplification on errors Background relocation Holistic reliability management Cold data need protection against retention errors least write amplification with redundancy likely to be identified by GC Selective redundancy for GC-ed data Read-hot data Need protection against disturbance errors number of data re-reads can be used as proxy likely to be identified by scrubber cost-benefit scrubbing Write-hot data no special attention required","title":"26 - SSD Reliability"},{"location":"csc568/26-SSD-Reliability/#design-tradeoffs-for-ssd-reliability","text":"","title":"Design Tradeoffs for SSD Reliability"},{"location":"csc568/26-SSD-Reliability/#high-level-objectives","text":"Understand the SSD-internal mechanisms behind fail-slow symptoms Examine SSD-internal reliability enhancement techniques Think about system- and device-level approaches for handling errors","title":"High-level objectives"},{"location":"csc568/26-SSD-Reliability/#ssds-reliability-issue","text":"How to make SSD reliable? Performance overhead? Across different chips and wear states?","title":"SSD's reliability issue"},{"location":"csc568/26-SSD-Reliability/#flash-memory-error-modeling","text":"","title":"Flash memory error modeling"},{"location":"csc568/26-SSD-Reliability/#ssd-reliability-enhancements","text":"Error correction code predictable performance is fixed at design-time Data re-reads is much more powerful than ECC increases latency for correcting erros Intra-SSD redundancy Protects against random and sporadic errors increases write amplification increases read amplification on errors Background relocation","title":"SSD reliability enhancements"},{"location":"csc568/26-SSD-Reliability/#holistic-reliability-management","text":"Cold data need protection against retention errors least write amplification with redundancy likely to be identified by GC Selective redundancy for GC-ed data Read-hot data Need protection against disturbance errors number of data re-reads can be used as proxy likely to be identified by scrubber cost-benefit scrubbing Write-hot data no special attention required","title":"Holistic reliability management"}]}